{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Model\n\nThis is the model file for the PyTorch implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.modules.utils import _pair\nfrom torch.nn import init\nimport math\nfrom net import MLP, StateTransition, GINTransition, GINPreTransition\n\n\nclass GNN(nn.Module):\n\n    def __init__(self, config, state_net=None, out_net=None):\n        super(GNN, self).__init__()\n\n        self.config = config\n        # hyperparameters and general properties\n        self.convergence_threshold = config.convergence_threshold\n        self.max_iterations = config.max_iterations\n        self.n_nodes = config.n_nodes\n        self.state_dim = config.state_dim\n        self.label_dim = config.label_dim\n        self.output_dim = config.output_dim\n        self.state_transition_hidden_dims = config.state_transition_hidden_dims\n        self.output_function_hidden_dims = config.output_function_hidden_dims\n\n        # node state initialization\n        self.node_state = torch.zeros(*[self.n_nodes, self.state_dim]).to(self.config.device)  # (n,d_n)\n        self.converged_states = torch.zeros(*[self.n_nodes, self.state_dim]).to(self.config.device)\n        # state and output transition functions\n        if state_net is None:\n            # self.state_transition_function = StateTransition(self.state_dim, self.label_dim,\n            #                                                  mlp_hidden_dim=self.state_transition_hidden_dims,\n            #                                                  activation_function=config.activation)\n            self.state_transition_function = GINPreTransition(self.state_dim, self.label_dim,\n                                                             mlp_hidden_dim=self.state_transition_hidden_dims,\n                                                             activation_function=config.activation)\n\n\n        else:\n            self.state_transition_function = state_net\n        if out_net is None:\n            self.output_function = MLP(self.state_dim, self.output_function_hidden_dims, self.output_dim)\n        else:\n            self.output_function = out_net\n\n        self.graph_based = self.config.graph_based\n\n    def reset_parameters(self):\n\n        self.state_transition_function.mlp.init()\n        self.output_function.init()\n\n    def forward(self,\n                edges,\n                agg_matrix,\n                node_labels,\n                node_states=None,\n                graph_agg=None\n                ):\n        n_iterations = 0\n        # convergence loop\n        # state initialization\n        node_states = self.node_state if node_states is None else node_states\n\n\n        # while n_iterations < self.max_iterations:\n        #     with torch.no_grad():  # without memory consumption\n        #         new_state = self.state_transition_function(node_states, node_labels, edges, agg_matrix)\n        #     n_iterations += 1\n        #     # convergence condition\n        #\n        #     # if torch.dist(node_states, new_state) < self.convergence_threshold:  # maybe uses broadcst?\n        #     #     break\n        #     # with torch.no_grad():\n        #         # distance = torch.sqrt(torch.sum((new_state - node_states) ** 2, 1) + 1e-20)\n        #     distance = torch.norm(input=new_state - node_states,\n        #                           dim=1)  # checked, they are the same (in cuda, some bug)\n        #     #\n        #     # diff =torch.norm(input=new_state - node_states, dim=1) -  torch.sqrt(torch.sum((new_state - node_states) ** 2, 1) )\n        #\n        #     check_min = distance < self.convergence_threshold\n        #     node_states = new_state\n        #\n        #     if check_min.all():\n        #         break\n        # node_states = self.state_transition_function(node_states, node_labels, edges, agg_matrix) # one more to propagate gradient only on last\n\n        while n_iterations < self.max_iterations:\n            new_state = self.state_transition_function(node_states, node_labels, edges, agg_matrix)\n            n_iterations += 1\n            # convergence condition\n            with torch.no_grad():\n                # distance = torch.sqrt(torch.sum((new_state - node_states) ** 2, 1) + 1e-20)\n                distance = torch.norm(input=new_state - node_states,\n                                      dim=1)  # checked, they are the same (in cuda, some bug)\n\n                check_min = distance < self.convergence_threshold\n            node_states = new_state\n\n            if check_min.all():\n                break\n\n        states = node_states\n        self.converged_states = states\n        if self.graph_based:\n            states = torch.matmul(graph_agg, node_states)\n\n        output = self.output_function(states)\n\n        return output, n_iterations"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}