{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nNet file\n========\n\nThis is the Net file for the subgraph problem: state and output transition function definition\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\nimport numpy as np\n\n\ndef weight_variable(shape, nm):\n    # function to initialize weights\n    initial = tf.truncated_normal(shape, stddev=0.1)\n    tf.summary.histogram(nm, initial, collections=['always'])\n    return tf.Variable(initial, name=nm)\n\n\nclass Net:\n    # class to define state and output network\n\n    def __init__(self, input_dim, state_dim, output_dim):\n        # initialize weight and parameter\n\n        self.EPSILON = 0.00000001\n\n        self.input_dim = input_dim\n        self.state_dim = state_dim\n        self.output_dim = output_dim\n        self.state_input = self.input_dim - 1 + state_dim  # removing the id_ dimension\n\n        #### TO BE SET ON A SPECIFIC PROBLEM\n        self.state_l1 = 15\n        self.state_l2 = self.state_dim\n\n        self.output_l1 = 10\n        self.output_l2 = self.output_dim\n\n    def netSt(self, inp):\n        with tf.variable_scope('State_net'):\n\n            layer1 = tf.layers.dense(inp, self.state_l1, activation=tf.nn.tanh)\n            layer2 = tf.layers.dense(layer1, self.state_l2, activation=tf.nn.tanh)\n\n            return layer2\n\n    def netOut(self, inp):\n\n            layer1 = tf.layers.dense(inp, self.output_l1, activation=tf.nn.tanh)\n            layer2 = tf.layers.dense(layer1, self.output_l2, activation=tf.nn.softmax)\n\n            return layer2\n\n    def Loss(self, output, target, output_weight=None):\n        # method to define the loss function\n        #lo = tf.losses.softmax_cross_entropy(target, output)\n        output = tf.maximum(output, self.EPSILON, name=\"Avoiding_explosions\")  # to avoid explosions\n        xent = -tf.reduce_sum(target * tf.log(output), 1)\n        lo = tf.reduce_mean(xent)\n        return lo\n\n    def Metric(self, target, output, output_weight=None):\n        # method to define the evaluation metric\n        correct_prediction = tf.equal(tf.argmax(output, 1), tf.argmax(target, 1))\n        metric = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n\n        return metric"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}