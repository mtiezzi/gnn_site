{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Net file\n\nThis is the net file for the PyTorch implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport typing\n\n\nclass MLP(nn.Module):\n    def __init__(self, input_dim, hidden_sizes: typing.Iterable[int], out_dim, activation_function=nn.Sigmoid(),\n                 activation_out=None):\n        super(MLP, self).__init__()\n\n        i_h_sizes = [input_dim] + hidden_sizes  # add input dim to the iterable\n        self.mlp = nn.Sequential()\n        for idx in range(len(i_h_sizes) - 1):\n            self.mlp.add_module(\"layer_{}\".format(idx),\n                                nn.Linear(in_features=i_h_sizes[idx], out_features=i_h_sizes[idx + 1]))\n            self.mlp.add_module(\"act\", activation_function)\n        self.mlp.add_module(\"out_layer\", nn.Linear(i_h_sizes[-1], out_dim))\n        if activation_out is not None:\n            self.mlp.add_module(\"out_layer_activation\", activation_out)\n\n    def init(self):\n        for i, l in enumerate(self.mlp):\n            if type(l) == nn.Linear:\n                nn.init.xavier_normal_(l.weight)\n\n    def forward(self, x):\n        return self.mlp(x)\n\n\n# code from Pedro H. Avelar\n\nclass StateTransition(nn.Module):\n\n    def __init__(self,\n                 node_state_dim: int,\n                 node_label_dim: int,\n                 mlp_hidden_dim: typing.Iterable[int],\n                 activation_function=nn.Tanh()\n                 ):\n        super(type(self), self).__init__()\n        d_i = node_state_dim + 2 * node_label_dim  # arc state computation f(l_v, l_n, x_n)\n        d_o = node_state_dim\n        d_h = list(mlp_hidden_dim)  # if already a list, no change\n        self.mlp = MLP(input_dim=d_i, hidden_sizes=d_h, out_dim=d_o, activation_function=activation_function,\n                       activation_out=activation_function)  # state transition function, non-linearity also in output\n\n    def forward(\n            self,\n            node_states,\n            node_labels,\n            edges,\n            agg_matrix,\n    ):\n        src_label = node_labels[edges[:, 0]]\n        tgt_label = node_labels[edges[:, 1]]\n        tgt_state = node_states[edges[:, 1]]\n        edge_states = self.mlp(\n            torch.cat(\n                [src_label, tgt_label, tgt_state],\n                -1\n            )\n        )\n\n        new_state = torch.matmul(agg_matrix, edge_states)\n        return new_state\n\n\n\nclass GINTransition(nn.Module):\n\n    def __init__(self,\n                 node_state_dim: int,\n                 node_label_dim: int,\n                 mlp_hidden_dim: typing.Iterable[int],\n                 activation_function=nn.Tanh()\n                 ):\n        super(type(self), self).__init__()\n        d_i = node_state_dim + node_label_dim\n        d_o = node_state_dim\n        d_h = list(mlp_hidden_dim)\n        self.mlp = MLP(input_dim=d_i, hidden_sizes=d_h, out_dim=d_o, activation_function=activation_function,\n                       activation_out=activation_function)  # state transition function, non-linearity also in output\n\n    def forward(\n            self,\n            node_states,\n            node_labels,\n            edges,\n            agg_matrix,\n\n    ):\n        state_and_label = torch.cat(\n            [node_states, node_labels],\n            -1\n        )\n        aggregated_neighbourhood = torch.matmul(agg_matrix, state_and_label[edges[:, 1]])\n        node_plus_neighbourhood = state_and_label + aggregated_neighbourhood\n        new_state = self.mlp(node_plus_neighbourhood)\n        return new_state\n\n\nclass GINPreTransition(nn.Module):\n\n    def __init__(self,\n                 node_state_dim: int,\n                 node_label_dim: int,\n                 mlp_hidden_dim: typing.Iterable[int],\n                 activation_function=nn.Tanh()\n                 ):\n        super(type(self), self).__init__()\n        d_i = node_state_dim +  node_label_dim\n        d_o = node_state_dim\n        d_h = list(mlp_hidden_dim)\n        self.mlp = MLP(input_dim=d_i, hidden_sizes=d_h, out_dim=d_o, activation_function=activation_function,\n                       activation_out=activation_function)\n\n    def forward(\n            self,\n            node_states,\n            node_labels,\n            edges,\n            agg_matrix,\n    ):\n        intermediate_states = self.mlp(\n            torch.cat(\n                [node_states, node_labels],\n                -1\n            )\n        )\n        new_state = (\n                torch.matmul(agg_matrix, intermediate_states[edges[:, 1]])\n                + torch.matmul(agg_matrix, intermediate_states[edges[:, 0]])\n        )\n        return new_state"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}